{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5dbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d9fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BandSpecificGraphSAGE(nn.Module):\n",
    "    \"\"\"Process a single band's sequence of temporal graphs\"\"\"\n",
    "    def __init__(self, in_channels=18, hidden_channels=64 #64\n",
    "    , dropout_rate=0.5, num_nodes=19):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # GraphSAGE layers for spatial feature extraction\n",
    "        self.sage1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.sage2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # Temporal processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_channels * num_nodes,\n",
    "            hidden_size=hidden_channels,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout= 0.0 #if 2 > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention for temporal features\n",
    "        self.temporal_attention = nn.Linear(hidden_channels * 2, 1)\n",
    "        \n",
    "        # Normalization and dropout\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_channels)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_channels * 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, graphs):\n",
    "        seq_embs = []\n",
    "        for graph in graphs:\n",
    "            # First GraphSAGE layer\n",
    "            x = self.sage1(graph.x, graph.edge_index)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.layer_norm1(x)\n",
    "            if self.training:\n",
    "                x = self.dropout(x)\n",
    "            \n",
    "            # Second GraphSAGE layer\n",
    "            x = self.sage2(x, graph.edge_index)\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "            if self.training:\n",
    "                x = self.dropout(x)\n",
    "            \n",
    "            # Flatten for temporal processing\n",
    "            x_flat = x.reshape(1, -1)\n",
    "            seq_embs.append(x_flat)\n",
    "        \n",
    "        if len(seq_embs) > 0:\n",
    "            seq_tensor = torch.cat(seq_embs, dim=0).unsqueeze(0)\n",
    "            lstm_out, _ = self.lstm(seq_tensor)\n",
    "            \n",
    "            # Apply temporal attention\n",
    "            lstm_out = self.layer_norm2(lstm_out)\n",
    "            attn_weights = F.softmax(self.temporal_attention(lstm_out).squeeze(-1), dim=1)\n",
    "            attn_applied = torch.bmm(attn_weights.unsqueeze(1), lstm_out)\n",
    "            output = attn_applied.squeeze(1)  # Shape: [1, hidden_channels*2]\n",
    "            return output\n",
    "        else:\n",
    "            device = next(self.parameters()).device\n",
    "            return torch.zeros(1, self.hidden_channels * 2, device=device)\n",
    "\n",
    "class MultiBandAttentionFusion(nn.Module):\n",
    "    \"\"\"Fusion model with improved multi-head attention for band fusion\"\"\"\n",
    "    def __init__(self, num_bands=5, hidden_channels=64, num_classes=2, dropout_rate=0.5, num_nodes=19, in_channels=18):\n",
    "        super().__init__()\n",
    "        self.num_bands = num_bands\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.band_processors = nn.ModuleList()\n",
    "        for band_idx in range(num_bands):\n",
    "            self.band_processors.append(\n",
    "                BandSpecificGraphSAGE(in_channels, hidden_channels, dropout_rate, num_nodes)\n",
    "            )\n",
    "        \n",
    "        self.band_projection = nn.Linear(hidden_channels * 2, hidden_channels * 2)\n",
    "        self.band_attention = nn.Linear(hidden_channels * 2, 1)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_channels * 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_channels * 2, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.band_weights_history = []\n",
    "        self.l1_factor = 0.01 #0.01 #0.05\n",
    "    \n",
    "    def forward(self, subjects_data):\n",
    "        all_outputs = []\n",
    "        all_band_weights = []\n",
    "        \n",
    "        for subject in subjects_data:\n",
    "            band_features = []\n",
    "            \n",
    "            for band_idx, band_graphs in enumerate(subject):\n",
    "                band_output = self.band_processors[band_idx](band_graphs)\n",
    "                band_output = self.band_projection(band_output)\n",
    "                band_features.append(band_output)\n",
    "            \n",
    "            band_features = torch.cat(band_features, dim=0)  # [num_bands, hidden_channels*2]\n",
    "            num_bands = band_features.shape[0]\n",
    "\n",
    "            attention_logits = self.band_attention(band_features).squeeze(-1)\n",
    "            attention_weights = F.softmax(attention_logits, dim=0)\n",
    "            \n",
    "            attended_features = torch.zeros_like(band_features[0]).unsqueeze(0)\n",
    "            for i in range(num_bands):\n",
    "                attended_features += band_features[i].unsqueeze(0) * attention_weights[i]\n",
    "            \n",
    "            all_band_weights.append(attention_weights.detach().cpu())\n",
    "\n",
    "            fused_features = self.layer_norm(attended_features)\n",
    "            out = self.fc1(fused_features)\n",
    "            out = F.relu(out)\n",
    "            if self.training:\n",
    "                out = self.dropout(out)\n",
    "            out = self.fc2(out)\n",
    "\n",
    "            if self.training and self.l1_factor > 0:\n",
    "                l1_loss = self.l1_factor * torch.abs(attention_weights - 1.0 / num_bands).sum()\n",
    "                self.l1_loss = l1_loss\n",
    "\n",
    "            all_outputs.append(out)\n",
    "        \n",
    "        if self.training:\n",
    "            self.band_weights_history.append(torch.stack(all_band_weights).mean(dim=0))\n",
    "        \n",
    "        return torch.cat(all_outputs, dim=0)\n",
    "    def get_band_importance(self):\n",
    "        \"\"\"Returns the average attention weight for each band over the training history\"\"\"\n",
    "        if not self.band_weights_history:\n",
    "            return None\n",
    "\n",
    "        all_weights = torch.stack(self.band_weights_history)  # shape: [num_epochs, num_bands]\n",
    "        mean_weights = all_weights.mean(dim=0)  # shape: [num_bands]\n",
    "        return mean_weights.cpu().numpy()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9651ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
