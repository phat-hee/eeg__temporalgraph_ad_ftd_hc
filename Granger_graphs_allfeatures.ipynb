{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting graphs:   0%|          | 0/88 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from numpy.linalg import svd\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "from scipy.signal import welch, csd, hilbert\n",
    "from scipy.integrate import simpson  # Changed from simps to simpson\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def get_label(s):\n",
    "    if 1 <= s <= 36:\n",
    "        return 2\n",
    "    elif 37 <= s <= 65:\n",
    "        return 0\n",
    "    elif 66 <= s <= 88:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def svd_entropy(sig, order=10, delay=1):\n",
    "    \"\"\"\n",
    "    Calculate SVD entropy of a 1D time series signal.\n",
    "    \"\"\"\n",
    "    N = len(sig)\n",
    "    if N < (order - 1) * delay + 1:\n",
    "        return 0.0\n",
    "    try:\n",
    "        X = np.array([sig[i: N - (order - 1) * delay + i: delay] for i in range(order)]).T\n",
    "        W = svd(X, compute_uv=False)\n",
    "        W /= W.sum()\n",
    "        return entropy(W, base=2)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def hjorth_parameters(sig):\n",
    "    \"\"\"Compute Hjorth parameters: activity, mobility, and complexity\"\"\"\n",
    "    first_deriv = np.diff(sig)\n",
    "    second_deriv = np.diff(first_deriv)\n",
    "\n",
    "    var_zero = np.var(sig)\n",
    "    var_d1 = np.var(first_deriv)\n",
    "    var_d2 = np.var(second_deriv)\n",
    "\n",
    "    activity = var_zero\n",
    "    mobility = np.sqrt(var_d1 / var_zero) if var_zero != 0 else 0\n",
    "    complexity = np.sqrt(var_d2 / var_d1) / mobility if var_d1 != 0 and mobility != 0 else 0\n",
    "    return activity, mobility, complexity\n",
    "\n",
    "def spectral_features(sig, fs=500):\n",
    "    f, psd = welch(sig, fs=fs)\n",
    "    mean_psd = np.mean(psd)\n",
    "    # Fixed simpson call - using correct parameter syntax\n",
    "    total_power = simpson(y=psd, x=f)\n",
    "    rel_power = total_power / (len(f) * np.max(psd)) if np.max(psd) > 0 else 0\n",
    "    psd_norm = psd / np.sum(psd) if np.sum(psd) > 0 else np.zeros_like(psd)\n",
    "    spec_entropy = entropy(psd_norm, base=2) if np.any(psd_norm > 0) else 0\n",
    "    return mean_psd, rel_power, spec_entropy    \n",
    "\n",
    "def shannon_entropy(sig, bins=4):\n",
    "    h, _ = np.histogram(sig, bins=bins)\n",
    "    if np.sum(h) == 0:\n",
    "        return 0.0\n",
    "    pd = h / np.sum(h)\n",
    "    pd = pd[pd > 0]\n",
    "    return entropy(pd, base=2) if len(pd) > 0 else 0.0\n",
    "\n",
    "def compute_node_signal_features(arr, fs=500):\n",
    "    n_channels = arr.shape[1]\n",
    "    feats = []\n",
    "    for ch in range(n_channels):\n",
    "        s = arr[:, ch]\n",
    "\n",
    "        mn = s.min()\n",
    "        mx = s.max()\n",
    "        ku = kurtosis(s)\n",
    "        sk_ = skew(s)\n",
    "        m = s.mean()\n",
    "        st = s.std()\n",
    "        sl = np.polyfit(np.arange(len(s)), s, 1)[0]\n",
    "        sh = shannon_entropy(s, bins=4)\n",
    "\n",
    "        mean_psd, rel_power, spec_entropy = spectral_features(s, fs)\n",
    "        activity, mobility, complexity = hjorth_parameters(s)\n",
    "        rms = np.sqrt(np.mean(s ** 2))\n",
    "        zc = ((s[:-1] * s[1:]) < 0).sum()\n",
    "        svd_ent = svd_entropy(s, order=10, delay=1)\n",
    "\n",
    "        feats.append([\n",
    "            mn, mx, ku, sk_, m, st, sl, sh,\n",
    "            mean_psd, rel_power, spec_entropy,\n",
    "            activity, mobility, complexity,\n",
    "            rms, zc, svd_ent\n",
    "        ])\n",
    "    return np.array(feats)\n",
    "\n",
    "\n",
    "def compute_node_hubscore(adj_matrix):\n",
    "    # Build undirected Nx.Graph from 'adj_matrix'\n",
    "    G = nx.from_numpy_array(adj_matrix, create_using=nx.Graph())\n",
    "    try:\n",
    "        hub_dict, _ = nx.hits(G, max_iter=1000, tol=1e-8, normalized=True)\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        hub_dict = {n: 0.0 for n in G.nodes()}\n",
    "    hub_vals = np.array([hub_dict[n] for n in G.nodes()], dtype=float)\n",
    "    return hub_vals  # shape [n_channels]\n",
    "\n",
    "def standard_coherence(x, y, fs=500):\n",
    "    f, Pxx = welch(x, fs=fs)\n",
    "    _, Pyy = welch(y, fs=fs)\n",
    "    _, Pxy = csd(x, y, fs=fs)\n",
    "    cxy = np.abs(Pxy)**2 / ((Pxx * Pyy) + 1e-12)\n",
    "    return np.mean(cxy)\n",
    "\n",
    "def compute_granger_matrix(data, max_lag=1):\n",
    "    \"\"\"\n",
    "    Compute Granger causality matrix for all channel pairs\n",
    "    Returns a matrix of p-values (lower = more significant causality)\n",
    "    \"\"\"\n",
    "    n_channels = data.shape[1]\n",
    "    granger_matrix = np.zeros((n_channels, n_channels))\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            if i != j:  # Skip self-loops\n",
    "                x = data[:, i]\n",
    "                y = data[:, j]\n",
    "                # Test if channel i Granger-causes channel j\n",
    "                arr = np.column_stack([y, x])  # y is affected by x\n",
    "                try:\n",
    "                    result = grangercausalitytests(arr, maxlag=max_lag, verbose=False)\n",
    "                    # Use 1 - p-value so higher values = stronger causality\n",
    "                    granger_val = 1.0 - result[max_lag][0]['params_ftest'][1]\n",
    "                    granger_matrix[i, j] = granger_val\n",
    "                except:\n",
    "                    granger_matrix[i, j] = 0.0\n",
    "    \n",
    "    return granger_matrix\n",
    "\n",
    "def single_lag_granger(x, y, max_lag=1):\n",
    "    arr = np.column_stack([y, x])  # Test if x Granger-causes y\n",
    "    try:\n",
    "        out = grangercausalitytests(arr, maxlag=max_lag, verbose=False)\n",
    "        return 1.0 - out[max_lag][0]['params_ftest'][1]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_plv_phase_lag(u, v, hilbert_cache):\n",
    "    ax = hilbert_cache[u]\n",
    "    ay = hilbert_cache[v]\n",
    "    px = np.angle(ax)\n",
    "    py = np.angle(ay)\n",
    "    diff = px - py\n",
    "    plv_val  = np.abs(np.mean(np.exp(1j * diff)))\n",
    "    phase_lg = np.mean(np.abs(diff))\n",
    "    return plv_val, phase_lg\n",
    "\n",
    "def compute_edge_feats(u, v, window_data, hilbert_cache, granger_matrix, partial_corr, fs=500):\n",
    "    x = window_data[:, u]\n",
    "    y = window_data[:, v]\n",
    "    plv_val, phase_lg = compute_plv_phase_lag(u, v, hilbert_cache)\n",
    "    coh_val   = standard_coherence(x, y, fs=fs)\n",
    "    cross_corr= np.corrcoef(x, y)[0,1]\n",
    "    gc_val    = granger_matrix[u, v]  # Use the precomputed Granger causality value\n",
    "    pcorr_val = partial_corr[u, v]\n",
    "    return [phase_lg, coh_val, cross_corr, plv_val, gc_val, pcorr_val]\n",
    "\n",
    "def parallel_edge_features(edges, window_data, hilbert_cache, granger_matrix, partial_corr, fs=500, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_edge_feats)(u, v, window_data, hilbert_cache, granger_matrix, partial_corr, fs)\n",
    "        for (u, v) in edges\n",
    "    )\n",
    "    return np.array(results)\n",
    "\n",
    "def make_graph_sparse(G, threshold):\n",
    "    to_remove = []\n",
    "    for u, v in G.edges():\n",
    "        w = abs(G[u][v]['weight'])\n",
    "        if w < threshold:\n",
    "            to_remove.append((u, v))\n",
    "    G.remove_edges_from(to_remove)\n",
    "    return G\n",
    "\n",
    "def main():\n",
    "    INPUT_DIR = r\"C:\\Users\\fathi\\Desktop\\Rest_eeg_ds004504-download\\derivatives\\500hz_bands_final\\beta\\numpy\"\n",
    "    SAVE_DIR   = r\"C:\\Users\\fathi\\Desktop\\Rest_eeg_ds004504-download\\derivatives\\500hz_bands_final\\beta\\numpy\\granger_temporal_v1_p95_OVERLAP50\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    FS = 500\n",
    "    WINDOW_SEC = 4\n",
    "    WINDOW_LEN = FS * WINDOW_SEC\n",
    "    PERCENTILE = 95\n",
    "    MAX_LAG = 1  # Maximum lag for Granger causality tests\n",
    "\n",
    "    dynamic_data = []\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    all_files = [\n",
    "        f for f in os.listdir(INPUT_DIR)\n",
    "        if f.endswith(\".npy\") and f.startswith(\"sub-\")\n",
    "    ]\n",
    "\n",
    "    for fn in tqdm(all_files, desc=\"Extracting graphs\"):\n",
    "        stem, _ = os.path.splitext(fn)\n",
    "        match = re.match(r\"sub-(\\d+)\", stem)\n",
    "        if not match:\n",
    "            continue\n",
    "        subj_id = int(match.group(1))\n",
    "\n",
    "        label_val = get_label(subj_id)\n",
    "        if label_val is None:\n",
    "            continue\n",
    "        label_tensor = torch.tensor([label_val], dtype=torch.long)\n",
    "\n",
    "        eeg_path = os.path.join(INPUT_DIR, fn)\n",
    "        time_series = np.load(eeg_path)\n",
    "        if time_series.shape[0] < time_series.shape[1]:\n",
    "            time_series = time_series.T\n",
    "\n",
    "        subject_data = []\n",
    "        time_len, n_channels = time_series.shape\n",
    "\n",
    "        stride = int(WINDOW_LEN * 0.5)  # 90% overlap\n",
    "\n",
    "\n",
    "        for start_idx in range(0, time_len - WINDOW_LEN + 1, stride):\n",
    "\n",
    "            window_data = time_series[start_idx : start_idx + WINDOW_LEN, :]\n",
    "\n",
    "            # 1) Compute Granger causality matrix\n",
    "            granger_matrix = compute_granger_matrix(window_data, max_lag=MAX_LAG)\n",
    "            \n",
    "            # No self-loops in Granger matrix\n",
    "            np.fill_diagonal(granger_matrix, 0.0)\n",
    "            \n",
    "            # 2) Build directed graph from Granger causality matrix\n",
    "            G = nx.from_numpy_array(granger_matrix, create_using=nx.DiGraph())\n",
    "            \n",
    "            # 3) Threshold to keep only significant causality\n",
    "            edgeweight = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "            thr_val = np.percentile(edgeweight, PERCENTILE) if len(edgeweight) > 0 else 0.0\n",
    "            sparse_G = make_graph_sparse(G, thr_val)\n",
    "            \n",
    "            edges = list(sparse_G.edges())  # Directed edges (u,v) where u Granger-causes v\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "            \n",
    "            # Compute partial correlation for edge features\n",
    "            measure = ConnectivityMeasure(kind='partial correlation')\n",
    "            pcorr = measure.fit_transform([window_data])[0]\n",
    "            np.fill_diagonal(pcorr, 0.0)\n",
    "            \n",
    "            # Adjacency for hubscore (convert to undirected for hub calculation)\n",
    "            nodelist = sorted(sparse_G.nodes())\n",
    "            adjacency_thresh = nx.to_numpy_array(sparse_G, nodelist=nodelist)\n",
    "            \n",
    "            # Node features\n",
    "            node_feats_signal = compute_node_signal_features(window_data, FS)\n",
    "            hub_vals = compute_node_hubscore(adjacency_thresh)  # Need undirected graph for HITS\n",
    "            combined_node_feats = np.hstack([node_feats_signal, hub_vals.reshape(-1,1)])\n",
    "            x_torch = torch.tensor(combined_node_feats, dtype=torch.float)\n",
    "            \n",
    "            # Edge features\n",
    "            hilb_cache = [hilbert(window_data[:, ch]) for ch in range(n_channels)]\n",
    "            ef_array = parallel_edge_features(edges, window_data, hilb_cache, granger_matrix, pcorr, fs=FS, n_jobs=-1)\n",
    "            ef_torch = torch.tensor(ef_array, dtype=torch.float)\n",
    "            \n",
    "            # Build Data object\n",
    "            data_obj = Data(\n",
    "                x=x_torch,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=ef_torch,\n",
    "                y=label_tensor\n",
    "            )\n",
    "            subject_data.append(data_obj)\n",
    "            \n",
    "        dynamic_data.append(subject_data)\n",
    "        \n",
    "        for w_i, d_obj in enumerate(subject_data):\n",
    "            out_name = f\"{stem}_win{w_i}.pt\"\n",
    "            out_path = os.path.join(SAVE_DIR, out_name)\n",
    "            torch.save(d_obj, out_path)\n",
    "    \n",
    "    torch.save(dynamic_data, \"granger_all_dynamic_data_ol50_alpha.pt\")\n",
    "    print(f\"\\nAll dynamic graphs extracted. dynamic_data has {len(dynamic_data)} subjects.\")\n",
    "    print(f\"Also saved each window .pt in {SAVE_DIR}\")\n",
    "    print(\"Saved entire memory object to granger_all_dynamic_data.pt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Node features (19 total):\n",
    "# 1. min, 2. max, 3. kurtosis, 4. skewness, 5. mean, 6. std, 7. slope, 8. shannon_entropy\n",
    "# 9. mean_psd, 10. relative_power, 11. spectral_entropy\n",
    "# 12. activity, 13. mobility, 14. complexity\n",
    "# 15. rms, 16. zero_crossings, 17. hubscore\n",
    "\n",
    "# Edge features (6 total):\n",
    "# 1. phase_lag, 2. coherence, 3. cross_corr, 4. plv, 5. granger_causality, 6. partial_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
